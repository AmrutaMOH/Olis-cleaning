import saspy
from IPython.display import HTML
import pandas as pd
import numpy as np
from datetime import date, datetime, timedelta
from asn1crypto._ffi import null

# read data directly from the github of JHU
def getData1():

    url = "https://raw.githubusercontent.com/CSSEGISandData/COVID-19/master/csse_covid_19_data/csse_covid_19_time_series/time_series_covid19_confirmed_global.csv"
    url2 ="https://raw.githubusercontent.com/CSSEGISandData/COVID-19/master/csse_covid_19_data/csse_covid_19_time_series/time_series_covid19_recovered_global.csv"
    covid = pd.read_csv(url, error_bad_lines=False)
    recrd = pd.read_csv(url2, error_bad_lines=False)
    covid_can = (covid[covid["Country/Region"].isin(["Canada"])]
                    .drop(["Lat","Long","Country/Region"],axis=1)
                    .set_index("Province/State").T
                    .drop(['Recovered'],axis=1))
    recrd_can = (recrd[recrd["Country/Region"].isin(["Canada"])]
                    .drop(["Lat","Long","Province/State"],axis=1)
                    .set_index("Country/Region").
                    T.rename({'Canada':'Recovered'},axis=1))
    
    covid_can = covid_can.merge(recrd_can,how='left',left_index=True,right_index=True)
    # covid_can.plot(figsize=(10,6))
    # plt.tick_params(axis="x",labelsize=12)
    # plt.tick_params(axis='y',labelsize=12)
    # plt.legend(prop={"size":10})
    # plt.title('Prevalence of COVID-19 in Canada over time, by Province as of: '+ covid_can.index[-1], fontsize=10)
    # plt.ylabel('Cumulative Frequency', fontsize=20)
    # plt.xlabel('Date', fontsize=10)
    # plt.xticks(list(range(len(covid_can.index))), covid_can.index, fontsize=10, rotation=90)
    # plt.grid(axis="y")
    # plt.show()
    countries = ["China", "Italy", "US", 'Korea, South', "France", "UK","Canada"]

    covid_countries = pd.DataFrame(columns=countries)
    for c in countries:
        covid_countries[c] = covid[covid["Country/Region"].isin([c])]\
            .drop(["Lat","Long","Province/State" ],axis=1)\
            .set_index("Country/Region").sum()

    covid_ts=pd.merge(covid_can[["Ontario","Recovered","Quebec"]], covid_countries, right_index=True, left_index=True)
    covid_ts.index=pd.to_datetime(covid_ts.index, format="%m/%d/%y").date
    
    covid_ts.columns = [ x.replace(" ", '_').replace(',','') for x in covid_ts.columns]
    return covid_ts

def getData2():

    data1 = pd.read_csv(open("covid_ts.csv") )
    data1.rename({'Unnamed: 0':'date'},inplace=True, axis=1 )
    data1["Date"] = pd.to_datetime(data1.date)
    data1.sort_values(['Date'], axis=0, inplace=True)
    data1.index = pd.DatetimeIndex(data1.Date)
    data1.columns = [ x.replace(" ", '_').replace(',','') for x in data1.columns]
 
    return data1

def getClosest(Xdata, laty, longy, nrows=1):
    ## Xdata is two column data with latitude longitude
    ## returns:boolean index with TRUE for nrows that have lowest distance
    ## Eg. data1[getClosest(data[['latitude','longitude']], lat1, long1)]
    dist = np.sqrt(np.add(
                (np.diff(Xdata.longitude- longy))**2,
                (np.diff(Xdata.latitude- laty))**2))
    # print("  == getClosest:", np.sort(dist))
    return dist <= (np.sort(dist)[nrows])

# NOTE: e.g
# getNearestSite('5373','inst_no','phunum')
# getNearestSite('M3N1N1', 'postal_code', 'csduid')
# getNearestSite('2242','phunum','phuname')
## fill in any missing csduid;

#############################################################################################
def getNearestSite(PivotValue, PivotCol='inst_no', colInterested='phunum', basedata=None):
    # print(PivotCol, " --- ", PivotValue)
    t1 = None
    try:
        t1= basedata[colInterested][ basedata[PivotCol] == PivotValue ]
        t1 = list(t1[~t1.isna()])
        # print("\t *** from inst_info",t1[0])
    except:
        # print('\t\texception2:')
        pass
    if (t1 is None) or len(t1)==0  :
        print('distance based :')
        try: 
            lat1 = basedata.latitude[basedata[PivotCol] == PivotValue ].min()
            lon1 = basedata.longitude[basedata[PivotCol] == PivotValue ].min()
            # print("lat = %f , long = %f"%(lat1,lon1))
            keeprows = basedata[PivotCol] != PivotValue
            leastRows = list(getClosest(basedata[keeprows], lat1, lon1))
            # print( " === leastrows = ", leastRows, " nrows:",sum(leastRows))
            if sum(leastRows) >0 :
                t1 = list(basedata[colInterested][keeprows].iloc[leastRows])
            else: 
                return None
        except:
            # print('\t\t exception 3:')
            return None
    return t1[0] if isinstance(t1,list)  else t1


if  __name__ == "__main__":

    import re
    ## read pHU census data generated by Saad
    # phucensus = pd.read_csv( open('D:\\Users\\HDSB\\Projects\\COVID\\Data\\PHU Census\\PHU Census 2016.csv'))
    # phucensus.columns = [x.lower() for x in phucensus.columns]
    # agePatt = re.compile('.*aged\D*\(\d\d\)\D+\(\d\d\)\D*', flags=re.IGNORECASE)
    # eduPatt=re.compile('[school|bachelor')

    # def attachtuple(x):
    #     return ( 'age', agePatt.sub('\1-\2',x)) if  agePatt.search()!=None else \
    #         if 
    
    ######################################################################################
    datapath = "D:\\Users\\HDSB\\Projects\\COVID\\Data"
    sasconn = saspy.SASsession(cfgfile='./rangrejja_sascfg.py', cfgname='winiom')

    mapset=pd.read_sas(open( datapath +'\\map_pstl_site_csd.sas7bdat','rb'), 
            format='sas7bdat', encoding='latin-1')
    mapset.columns = [x.lower() for x in mapset.columns]
    mapset.dtypes
    set(mapset.inst_no)
    set(mapset.phunum)
    set(mapset.csduid)
    print(mapset.dtypes)

    ## LINDA's datasets of geographical mapping for COVID;
    ## Note that Linda's Data has weird Lat and Long info.
    ##      obtained from A_COVID.covid_institutions_all'
    ## get the institution from BCS
    inst_info = pd.read_sas(open(datapath +'\\bcs_map.sas7bdat','rb'),
            format='sas7bdat', encoding='latin-1')
    inst_info.columns = [x.lower() for x in inst_info.columns]
    # fix postal code columns if any \D in place of \d
    inst_info.postal_code.apply(lambda x: False if re.search(str(x),'\D\D') is None else True).sum()
    # inst_info.phunum= pd.to_numeric(inst_info.phunum,errors='ignore').fillna(0,downcast='infer').astype('object')
    set(inst_info.inst_no)
    inst_info[inst_info.phunum.isna()]
    inst_info.latitude = inst_info.latitude.round(7)
    inst_info.longitude = inst_info.longitude.round(7)
    inst_info.phuname = inst_info.phuname.str.upper()
    naphu = inst_info.phuname.isin(['.','','MISSING'])
    inst_info.phuname[naphu] = None
    inst_info.phuname[inst_info.phuname.isna()] = None
    

    # e.g
    # sum(getClosest(inst_info[['latitude','longitude']], 43.5, -77.2, 5))
    # fill in PHU's 


    naphu = inst_info.phunum.isna()
    inst_info.phunum[naphu]=( inst_info.inst_no[naphu]
                        .map(lambda x: getNearestSite(x,'inst_no','phunum',basedata=inst_info)))
    inst_info.phunum[naphu]

    ## fill in missing CSDUID
    nacsd = inst_info.csduid.isna()
    try :
        inst_info.csduid[nacsd] = (inst_info.postal_code[nacsd]
                                .map(lambda x:getNearestSite(x,'postal_code','csduid',inst_info)))
    except IndexError as idxerr:
        print("error : ", idxerr)
        inst_info.csduid[nacsd] = (inst_info.inst_no[nacsd]
                                .map(lambda x:getNearestSite(x,'inst_no','csduid',inst_info)))
    inst_info.csduid[nacsd]
    ## fill in missing postal_code
    napscode = inst_info.postal_code.isna()
    inst_info.postal_code[napscode] = (inst_info.inst_no[napscode]
                            .map(lambda x:getNearestSite(x,'inst_no','postal_code',inst_info)))
    inst_info.postal_code[napscode]

    # fill in missing PHUNames
    naphu = inst_info.phuname.isna()
    inst_info.phuname[naphu]=( inst_info.phunum[naphu]
                        .map(lambda x: getNearestSite(x,'phunum','phuname',basedata=inst_info)))
    inst_info.phuname[naphu]

    # WRITE THE OUTPUT TO THE MAPPING FILE inside MODELLER DATA folder
    inst_info.to_excel(datapath + '\\geomapping\\PHU_inst_map.xlsx', sheet_name='phu_map',index=False)
    

    ########################################################################################
    # get BCS map of inst_no to site_id;
    sasstmt = "proc sort data=BCS.ALC_CCO(keep=inst_no site_id) nodup  out=tmp1;\
                 by site_id; run"
    out1 = sasconn.submit(sasstmt)
    bcsmap = sasconn.sasdata('tmp1','WORK').to_df()
    bcsmap.columns = [x.lower() for x in bcsmap.columns]
    # bcsmap = bcsmap.astype('object',errors='raise')
    bcsmap = bcsmap.astype('int64', errors='ignore').fillna(0,downcast='infer')
    print(bcsmap.dtypes)
    # merge mapset with bcsmap to get csduid connected to site_id;
    t1 = inst_info.merge(bcsmap, how='outer', on=['inst_no','site_id'])
    mapset.site_id = mapset.site_id.astype('int64',errors='ignore').fillna(0,downcast='infer')

    #Read the cross walk file from the Mike Pacey;
    inst_info = pd.read_excel(datapath + '\\geomapping\\PHU_inst_map.xlsx')

    crosswalk = pd.read_excel(open(datapath + '\\geomapping'+
            '\\LTC Assessment_PHU and LTCIB Master List v4_April 18, 2020.xlsx', 'rb'),
            'PHU LTCIB Master List ',header=0 )
    crosswalk.head()
    crosswalk.columns = [x.lower() for x in crosswalk.columns]
    crosswalk.columns
    crosswalk.rename( columns={'postal code':'postal_code','phu ':'phuname','ltch #': 'inst_no',
        'home name':'hospital_name','city':'community' }, inplace=True,)
    selectcols = ['postal_code', 'phuname', 'inst_no','hospital_name','community']
    crosswalk = crosswalk[selectcols]
    pp1 = re.compile(' *DISTRICT *| *HEALTH UNIT *| *CITY OF|AREA *|(?<=REGION)AL')
    crosswalk.phuname = crosswalk.phuname.str.upper().apply(lambda x: pp1.sub('',str(x))).str.strip()
    crosswalk.postal_code = crosswalk.postal_code.str.replace(' ','').str.upper()
    crosswalk.inst_no  =crosswalk.inst_no.astype('str',errors='ignore').str.upper()
    crosswalk['inst_no'][crosswalk.inst_no == 'NAN'] = None
    set(crosswalk.inst_no) -set(inst_info.inst_no)
    crosswalk[crosswalk.inst_no.isna()]
    set(crosswalk.phuname) 
    set(inst_info.phuname)
    crosswalk = crosswalk.merge(inst_info[['phunum','phuname']].drop_duplicates(),how='left',on='phuname').drop_duplicates()
    crosswalk.shape
    # fill in missing phunums
    naphu = crosswalk.phunum.isna()
    crosswalk.postal_code[naphu]
    getNearestSite('Carleton Place','community','phunum')
    crosswalk.phunum[naphu] = (crosswalk.community[naphu]
                            .map(lambda x:getNearestSite(x,'community','phunum')))
    crosswalk.to_excel(datapath + '\\geomapping\\crosswalk.xlsx', 'crosswalk')

    ### REad in predictions from the LTCH level risk of outbreak 
    ltcrisk =pd.read_csv(datapath+'\\Modelling Data\\Automation\\haoran_home_outbreak_risk.csv')
    ltcrisk.columns
    ltcrisk.rename(columns={'Home Legal Name':'hospital_name'},inplace=True)
 
    crosswalk = pd.read_sas(datapath + '\\geomapping\\crosswalk_LTCH.sas7bdat')
    t1 = ltcrisk.merge(crosswalk, how='left',on='hospital_name')
    t2 = ltcrisk.merge(crosswalk, how='right',on='hospital_name')
    t2 = t2[t2.MasterNo.isna()].hospital_name
      

    def FindBestMatch(string, inSeries):
        matchstr = None
        print("string: ",string, inSeries)
        matchlist = inSeries.map(lambda x: str(x).upper() in str(string).upper())
        matchstr = inSeries[matchlist].tolist()
        return matchstr[0] if (matchstr != None and len(matchstr)>0) else string
    ltcrisk.hospital_name[t1.phuname.isnull()]= t1[t1.phuname.isnull()].hospital_name.apply(FindBestMatch,args=(t2,))

    t1 = ltcrisk.merge(crosswalk, how='left',on='hospital_name')

    t1.to_csv( datapath+"\\Modelling Data\\Automation\\Home_outbreak_risk_wPHU.txt",index=False, sep="\t")

    ## get case data that admitted to any bed;
    ## NOTE: seems like BCS_PATIENTS have inst# that are format 5XXX which doesn't align
    ##      with any of that i have here.
    bcsipcov = sasconn.sasdata('COVID19_INPATIENT_DAILY', 'BCS').to_df()
    bcsipcov.columns = [ x.lower() for x in bcsip.columns ]
 

    # bcs ip beds data;
    bcsip = sasconn.sasdatA('BCS_HUT','BCS').to_df()

    # site daily update
    ccis = sasconn.sasdata('CCSO_SITE_'+(date.today()-timedelta(1)).strftime("%d%b%Y"), 'A0COVID')
    # ccso = ccso.to_df()
    # ccso.columns = [x.lower() for x in ccso.columns]

    ## read in iphis data;
    iphis = sasconn.sasdata('iphis_report','A0COVID')
    iphis = iphis.to_df()
    iphis.columns = [x.lower() for x in iphis.columns]
    iphis.client_id.unique().shape
    
    
    